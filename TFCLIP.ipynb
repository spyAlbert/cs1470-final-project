{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Dataset preprocess (COCO)"
      ],
      "metadata": {
        "id": "oO7fJQbQ_oTq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import random\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "from skimage import io as skio\n",
        "from PIL import Image\n",
        "\n",
        "def load_json(path: Path):\n",
        "    \"\"\"Load captions JSON file into memory.\"\"\"\n",
        "    with path.open(\"r\") as fp:\n",
        "        captions = json.load(fp)\n",
        "    print(f\"{len(captions)} captions loaded from json\")\n",
        "    return captions\n",
        "\n",
        "def sample_dataset(captions, fraction=0.1):\n",
        "    \"\"\"Randomly sample a fraction of the dataset.\"\"\"\n",
        "    sample_size = int(len(captions) * fraction)\n",
        "    sampled_captions = random.sample(captions, sample_size)\n",
        "    print(f\"Sampled {sample_size} captions from the dataset.\")\n",
        "    return sampled_captions\n",
        "\n",
        "def create_new_dirs(base_path: Path):\n",
        "    \"\"\"Create directories for the new dataset.\"\"\"\n",
        "    os.makedirs(base_path / \"train2014\", exist_ok=True)\n",
        "    os.makedirs(base_path / \"val2014\", exist_ok=True)\n",
        "    os.makedirs(base_path / \"annotations\", exist_ok=True)\n",
        "    print(f\"Created new directories at {base_path}\")\n",
        "\n",
        "def find_image(img_id: int, train_path: Path, val_path: Path) -> Path:\n",
        "    \"\"\"Find image in train2014 or val2014 based on image_id.\"\"\"\n",
        "    try:\n",
        "        img_id = int(img_id)\n",
        "    except ValueError:\n",
        "        raise ValueError(f\"Image id {img_id} is not a valid integer.\")\n",
        "\n",
        "    filename = f\"COCO_train2014_{img_id:012d}.jpg\"\n",
        "    train_image_path = train_path / filename\n",
        "    if train_image_path.is_file():\n",
        "        return train_image_path\n",
        "\n",
        "    filename = f\"COCO_val2014_{img_id:012d}.jpg\"\n",
        "    val_image_path = val_path / filename\n",
        "    if val_image_path.is_file():\n",
        "        return val_image_path\n",
        "\n",
        "    raise FileNotFoundError(f\"Image {img_id} not found in train/val splits.\")\n",
        "\n",
        "def copy_image(img_path: Path, new_dir: Path):\n",
        "    \"\"\"Copy image to the new directory.\"\"\"\n",
        "    shutil.copy(img_path, new_dir)\n",
        "\n",
        "def preprocess_and_copy_dataset(captions, output_dir: Path, train_path: Path, val_path: Path):\n",
        "    \"\"\"Preprocess the dataset: sample 1/10th, copy images, and store captions.\"\"\"\n",
        "    sampled_captions = sample_dataset(captions, fraction=0.1)\n",
        "    create_new_dirs(output_dir)\n",
        "\n",
        "    new_train_dir = output_dir / \"train2014\"\n",
        "    new_val_dir = output_dir / \"val2014\"\n",
        "\n",
        "    new_captions = []\n",
        "\n",
        "    for item in sampled_captions:\n",
        "        img_id = item[\"image_id\"]\n",
        "        try:\n",
        "            img_path = find_image(img_id, train_path, val_path)\n",
        "        except FileNotFoundError:\n",
        "            continue\n",
        "\n",
        "        if \"train\" in str(img_path):\n",
        "            new_img_dir = new_train_dir\n",
        "        else:\n",
        "            new_img_dir = new_val_dir\n",
        "\n",
        "        copy_image(img_path, new_img_dir)\n",
        "        new_captions.append(item)\n",
        "\n",
        "    annotations_path = output_dir / \"annotations\" / \"train_caption.json\"\n",
        "    with annotations_path.open(\"w\") as fp:\n",
        "        json.dump(new_captions, fp)\n",
        "    print(f\"Saved {len(new_captions)} sampled captions to {annotations_path}\")\n",
        "\n",
        "def main():\n",
        "    original_train_path = Path(\"./data/coco/train2014\")\n",
        "    original_val_path = Path(\"./data/coco/val2014\")\n",
        "    output_dir = Path(\"./data/coco_sampled\")\n",
        "\n",
        "    caption_file = Path(\"train_caption.json\")\n",
        "    captions = load_json(caption_file)\n",
        "\n",
        "    preprocess_and_copy_dataset(captions, output_dir, original_train_path, original_val_path)\n",
        "    print(\"Preprocessing and dataset reduction complete!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3jTlI9J_s4K",
        "outputId": "ba4e9fcc-d31b-452e-f1ac-973a18c63990"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "56674 captions loaded from json\n",
            "Sampled 5667 captions from the dataset.\n",
            "Created new directories at data/coco_sampled\n",
            "Saved 0 sampled captions to data/coco_sampled/annotations/train_caption.json\n",
            "Preprocessing and dataset reduction complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clip preprocess using HugginFace"
      ],
      "metadata": {
        "id": "uzOquaoydyeN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import pickle\n",
        "import argparse\n",
        "from pathlib import Path\n",
        "\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from transformers import CLIPProcessor, TFCLIPModel\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def load_json(path: Path):\n",
        "    \"\"\"Read caption json to memory.\"\"\"\n",
        "    with path.open(\"r\") as fp:\n",
        "        captions = json.load(fp)\n",
        "    print(f\"{len(captions)} captions loaded from json\")\n",
        "    return captions\n",
        "\n",
        "def find_image(img_id: int) -> Path:\n",
        "    \"\"\"\n",
        "    Given a COCO image id, locate the JPG file in either train2014 or val2014 folder.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        img_id = int(img_id)\n",
        "    except ValueError:\n",
        "        raise ValueError(f\"Image id {img_id} is not a valid integer.\")\n",
        "\n",
        "    filename = f\"COCO_train2014_{img_id:012d}.jpg\"\n",
        "    train_path = Path(\"./data/coco_sampled/train2014\") / filename\n",
        "    if train_path.is_file():\n",
        "        return train_path\n",
        "\n",
        "    filename = f\"COCO_val2014_{img_id:012d}.jpg\"\n",
        "    val_path = Path(\"./data/coco_sampled/val2014\") / filename\n",
        "    if val_path.is_file():\n",
        "        return val_path\n",
        "\n",
        "    #raise FileNotFoundError(f\"Image {img_id} not found in train/val splits.\")\n",
        "\n",
        "def extract_embeddings(model, processor, captions) -> tuple[list[np.ndarray], list[dict]]:\n",
        "    \"\"\"\n",
        "    Iterate over captions, load corresponding images,\n",
        "    and compute CLIP image embeddings.\n",
        "    \"\"\"\n",
        "    features, meta = [], []\n",
        "\n",
        "    for idx in tqdm(range(len(captions))):\n",
        "        item = captions[idx]\n",
        "        img_path = find_image(item[\"image_id\"])\n",
        "        if img_path is None:\n",
        "            tqdm.write(f\"Skipping missing image {item['image_id']}\")\n",
        "            continue\n",
        "\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        inputs = processor(images=image, return_tensors=\"tf\")\n",
        "\n",
        "        outputs = model.get_image_features(**inputs)\n",
        "        emb = outputs.numpy()\n",
        "\n",
        "        item[\"clip_embedding\"] = idx\n",
        "        features.append(emb)\n",
        "        meta.append(item)\n",
        "\n",
        "        # checkpoint every 10k samples\n",
        "        if (idx + 1) % 10_000 == 0:\n",
        "            yield idx + 1, features, meta\n",
        "            features, meta = [], []\n",
        "\n",
        "    # final remainder\n",
        "    yield len(captions), features, meta\n",
        "\n",
        "def dump_checkpoint(out_path: Path, feats: list[np.ndarray], infos: list[dict]):\n",
        "    \"\"\"Serialize current batch to a pkl file.\"\"\"\n",
        "    tensor_cat = np.concatenate(feats, axis=0)\n",
        "    payload = {\"clip_embedding\": tensor_cat, \"captions\": infos}\n",
        "    with out_path.open(\"wb\") as fp:\n",
        "        pickle.dump(payload, fp)\n",
        "\n",
        "def run(model_name: str):\n",
        "    # Use GPU if available\n",
        "    print(\"Num GPUs Available:\", len(tf.config.list_physical_devices('GPU')))\n",
        "\n",
        "    from transformers import TFAutoModel, AutoTokenizer\n",
        "\n",
        "    model_name = \"openai/clip-vit-base-patch32\"  # Correct Hugging Face model ID\n",
        "\n",
        "    model = TFAutoModel.from_pretrained(model_name)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    processor = CLIPProcessor.from_pretrained(model_name)\n",
        "\n",
        "    output_pkl = Path(f\"./data/coco/oscar_split_{model_name.replace('/', '_')}_train.pkl\")\n",
        "\n",
        "    caption_file = Path(\"train_caption.json\")\n",
        "    captions = load_json(caption_file)\n",
        "\n",
        "    progress = 0\n",
        "    for progress, feats, infos in extract_embeddings(model, processor, captions):\n",
        "        dump_checkpoint(output_pkl, feats, infos)\n",
        "\n",
        "    print(\"Done\")\n",
        "    print(f\"{progress} embeddings saved to {output_pkl}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import argparse\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--clip_model_name', type=str, default=\"ViT-B/32\")\n",
        "    args = parser.parse_known_args()[0]\n",
        "\n",
        "    # call your main function here with args\n",
        "    clip_model_name = args.clip_model_name\n",
        "    run(args.clip_model_name)"
      ],
      "metadata": {
        "id": "PmVrqCZWjFlW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "predict.py:"
      ],
      "metadata": {
        "id": "SiQkWuE8ixgD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from transformers import CLIPProcessor, TFAutoModelForImageClassification, GPT2Tokenizer\n",
        "import numpy as np\n",
        "import skimage.io as io\n",
        "import PIL.Image\n",
        "!pip install cog\n",
        "!pip install --upgrade cog\n",
        "#import cog\n",
        "\n",
        "# Model weights and paths\n",
        "WEIGHTS_PATHS = {\n",
        "    \"coco\": \"coco_weights.h5\",\n",
        "    \"conceptual-captions\": \"conceptual_weights.h5\",\n",
        "}\n",
        "\n",
        "class Predictor:\n",
        "    def __init__(self):\n",
        "        \"\"\"Load the model into memory to make running multiple predictions efficient\"\"\"\n",
        "        self.device = \"GPU\"  # or \"CPU\" if needed\n",
        "        self.processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "        self.clip_model = TFAutoModelForImageClassification.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "        self.tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "        self.models = {}\n",
        "        self.prefix_length = 10\n",
        "\n",
        "    def predict(self, image_path: str):\n",
        "        \"\"\"Run inference on an image.\"\"\"\n",
        "        # Load and preprocess the image\n",
        "        image = io.imread(image_path)\n",
        "        inputs = self.processor(images=image, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "        # Run the image through the model\n",
        "        with tf.device(self.device):\n",
        "            outputs = self.clip_model(**inputs)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "\n",
        "\n",
        "class ClipCaptionModel(tf.keras.Model):\n",
        "    def __init__(self, prefix_length: int, prefix_size: int = 512):\n",
        "        super(ClipCaptionModel, self).__init__()\n",
        "        self.prefix_length = prefix_length\n",
        "        self.gpt = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "        self.gpt_embedding_size = self.gpt.transformer.wte.weight.shape[1]\n",
        "        self.clip_project = tf.keras.layers.Dense(self.gpt_embedding_size * prefix_length)\n",
        "\n",
        "    def call(self, tokens, prefix, mask=None, labels=None):\n",
        "        embedding_text = self.gpt.transformer.wte(tokens)\n",
        "        prefix_projections = self.clip_project(prefix)\n",
        "        embedding_cat = tf.concat([prefix_projections, embedding_text], axis=1)\n",
        "\n",
        "        if labels is not None:\n",
        "            dummy_token = tf.zeros_like(tokens)\n",
        "            labels = tf.concat([dummy_token, tokens], axis=1)\n",
        "\n",
        "        out = self.gpt(inputs_embeds=embedding_cat, labels=labels, attention_mask=mask)\n",
        "        return out\n",
        "\n",
        "\n",
        "def generate_beam(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    beam_size=5,\n",
        "    prompt=None,\n",
        "    embed=None,\n",
        "    entry_length=67,\n",
        "    temperature=1.0,\n",
        "    stop_token=\".\",\n",
        "):\n",
        "    model.eval()\n",
        "    stop_token_index = tokenizer.encode(stop_token)[0]\n",
        "    tokens = None\n",
        "    scores = None\n",
        "    device = next(model.parameters()).device\n",
        "    seq_lengths = tf.ones(beam_size)\n",
        "    is_stopped = tf.zeros(beam_size, dtype=tf.bool)\n",
        "\n",
        "    if embed is not None:\n",
        "        generated = embed\n",
        "    else:\n",
        "        tokens = tokenizer.encode(prompt)\n",
        "        tokens = tf.convert_to_tensor(tokens)\n",
        "        generated = model.gpt.transformer.wte(tokens)\n",
        "\n",
        "    for i in range(entry_length):\n",
        "        outputs = model.gpt(inputs_embeds=generated)\n",
        "        logits = outputs.logits\n",
        "        logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n",
        "        logits = tf.nn.log_softmax(logits, axis=-1)\n",
        "\n",
        "        if scores is None:\n",
        "            scores, next_tokens = tf.math.top_k(logits, k=beam_size)\n",
        "            generated = tf.expand_dims(generated, axis=0)\n",
        "            next_tokens, scores = tf.transpose(next_tokens), scores[0]\n",
        "\n",
        "            tokens = next_tokens\n",
        "        else:\n",
        "            logits[is_stopped] = -float(\"inf\")\n",
        "            logits[is_stopped, 0] = 0\n",
        "            scores_sum = scores[:, None] + logits\n",
        "            seq_lengths[~is_stopped] += 1\n",
        "            scores_sum_average = scores_sum / seq_lengths[:, None]\n",
        "            scores_sum_average, next_tokens = tf.math.top_k(scores_sum_average, k=beam_size)\n",
        "\n",
        "            next_tokens_source = next_tokens // scores_sum.shape[1]\n",
        "            seq_lengths = seq_lengths[next_tokens_source]\n",
        "            next_tokens = next_tokens % scores_sum.shape[1]\n",
        "            tokens = next_tokens\n",
        "\n",
        "            generated = tf.concat([generated, next_token_embed], axis=1)\n",
        "            is_stopped = is_stopped + next_tokens.eq(stop_token_index).squeeze()\n",
        "            if is_stopped.all():\n",
        "                break\n",
        "\n",
        "    output_texts = [tokenizer.decode(output) for output in tokens]\n",
        "    return output_texts\n",
        "\n",
        "\n",
        "def generate2(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    tokens=None,\n",
        "    prompt=None,\n",
        "    embed=None,\n",
        "    entry_count=1,\n",
        "    entry_length=67,\n",
        "    top_p=0.8,\n",
        "    temperature=1.0,\n",
        "    stop_token=\".\",\n",
        "):\n",
        "    model.eval()\n",
        "    generated_list = []\n",
        "    stop_token_index = tokenizer.encode(stop_token)[0]\n",
        "\n",
        "    if embed is not None:\n",
        "        generated = embed\n",
        "    else:\n",
        "        tokens = tokenizer.encode(prompt)\n",
        "        tokens = tf.convert_to_tensor(tokens)\n",
        "        generated = model.gpt.transformer.wte(tokens)\n",
        "\n",
        "    for i in range(entry_length):\n",
        "        outputs = model.gpt(inputs_embeds=generated)\n",
        "        logits = outputs.logits\n",
        "        logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n",
        "\n",
        "        sorted_logits, sorted_indices = tf.math.top_k(logits, k=beam_size)\n",
        "        cumulative_probs = tf.cumsum(tf.nn.softmax(sorted_logits, axis=-1), axis=-1)\n",
        "        sorted_indices_to_remove = cumulative_probs > top_p\n",
        "        sorted_indices_to_remove[:, 1:] = sorted_indices_to_remove[:, :-1]\n",
        "        sorted_indices_to_remove[:, 0] = 0\n",
        "\n",
        "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
        "        logits[:, indices_to_remove] = -float(\"Inf\")\n",
        "\n",
        "        next_token = tf.argmax(logits, axis=-1)\n",
        "        next_token_embed = model.gpt.transformer.wte(next_token)\n",
        "\n",
        "        tokens = tf.concat([tokens, next_token], axis=1)\n",
        "        generated = tf.concat([generated, next_token_embed], axis=1)\n",
        "\n",
        "        if next_token == stop_token_index:\n",
        "            break\n",
        "\n",
        "    output_text = tokenizer.decode(tokens)\n",
        "    generated_list.append(output_text)\n",
        "\n",
        "    return generated_list[0]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yiOM1BvvjmR7",
        "outputId": "cc65ccd3-158d-4dbc-d233-b31f3d28bb36"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: cog in /usr/local/lib/python3.11/dist-packages (0.14.7)\n",
            "Requirement already satisfied: attrs<24,>=20.1 in /usr/local/lib/python3.11/dist-packages (from cog) (23.2.0)\n",
            "Requirement already satisfied: fastapi<0.116.0,>=0.100 in /usr/local/lib/python3.11/dist-packages (from cog) (0.115.12)\n",
            "Requirement already satisfied: pydantic<3,>=1.9 in /usr/local/lib/python3.11/dist-packages (from cog) (2.11.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from cog) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from cog) (2.32.3)\n",
            "Requirement already satisfied: structlog<25,>=20 in /usr/local/lib/python3.11/dist-packages (from cog) (24.4.0)\n",
            "Requirement already satisfied: typing_extensions>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from cog) (4.13.2)\n",
            "Requirement already satisfied: uvicorn<1,>=0.12 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]<1,>=0.12->cog) (0.34.2)\n",
            "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi<0.116.0,>=0.100->cog) (0.46.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9->cog) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9->cog) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9->cog) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->cog) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->cog) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->cog) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->cog) (2025.1.31)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn<1,>=0.12->uvicorn[standard]<1,>=0.12->cog) (8.1.8)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn<1,>=0.12->uvicorn[standard]<1,>=0.12->cog) (0.14.0)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]<1,>=0.12->cog) (0.6.4)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]<1,>=0.12->cog) (1.1.0)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]<1,>=0.12->cog) (0.21.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]<1,>=0.12->cog) (1.0.5)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]<1,>=0.12->cog) (15.0.1)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.47.0,>=0.40.0->fastapi<0.116.0,>=0.100->cog) (4.9.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi<0.116.0,>=0.100->cog) (1.3.1)\n",
            "Requirement already satisfied: cog in /usr/local/lib/python3.11/dist-packages (0.14.7)\n",
            "Requirement already satisfied: attrs<24,>=20.1 in /usr/local/lib/python3.11/dist-packages (from cog) (23.2.0)\n",
            "Requirement already satisfied: fastapi<0.116.0,>=0.100 in /usr/local/lib/python3.11/dist-packages (from cog) (0.115.12)\n",
            "Requirement already satisfied: pydantic<3,>=1.9 in /usr/local/lib/python3.11/dist-packages (from cog) (2.11.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from cog) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from cog) (2.32.3)\n",
            "Requirement already satisfied: structlog<25,>=20 in /usr/local/lib/python3.11/dist-packages (from cog) (24.4.0)\n",
            "Requirement already satisfied: typing_extensions>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from cog) (4.13.2)\n",
            "Requirement already satisfied: uvicorn<1,>=0.12 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]<1,>=0.12->cog) (0.34.2)\n",
            "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi<0.116.0,>=0.100->cog) (0.46.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9->cog) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9->cog) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9->cog) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->cog) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->cog) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->cog) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->cog) (2025.1.31)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn<1,>=0.12->uvicorn[standard]<1,>=0.12->cog) (8.1.8)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn<1,>=0.12->uvicorn[standard]<1,>=0.12->cog) (0.14.0)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]<1,>=0.12->cog) (0.6.4)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]<1,>=0.12->cog) (1.1.0)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]<1,>=0.12->cog) (0.21.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]<1,>=0.12->cog) (1.0.5)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]<1,>=0.12->cog) (15.0.1)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.47.0,>=0.40.0->fastapi<0.116.0,>=0.100->cog) (4.9.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi<0.116.0,>=0.100->cog) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "train.py:"
      ],
      "metadata": {
        "id": "PeMO08yYlXoz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from transformers import GPT2Model, GPT2Tokenizer, CLIPModel, CLIPProcessor\n",
        "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer\n",
        "import pickle\n",
        "import os\n",
        "import argparse\n",
        "import json\n",
        "from typing import Tuple, Optional\n",
        "from typing import Union\n",
        "from enum import Enum\n",
        "from tensorflow.keras import layers, Model\n",
        "from tensorflow.keras.activations import tanh, relu\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import sys\n",
        "from tqdm import tqdm\n",
        "\n",
        "class MappingType(Enum):\n",
        "    MLP = 'mlp'\n",
        "    Transformer = 'transformer'\n",
        "\n",
        "\n",
        "class ClipCocoDataset:\n",
        "    def __init__(self, data_path: str, prefix_length: int, normalize_prefix=False):\n",
        "        # Initialize tokenizer and dataset parameters\n",
        "        self.processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
        "        self.prefix_length = prefix_length\n",
        "        self.normalize_prefix = normalize_prefix\n",
        "\n",
        "        # Load dataset\n",
        "        with open(data_path, 'rb') as f:\n",
        "            all_data = pickle.load(f)\n",
        "        print(\"Data size is %0d\" % len(all_data[\"clip_embedding\"]))\n",
        "\n",
        "        self.prefixes = all_data[\"clip_embedding\"]\n",
        "        captions_raw = all_data[\"captions\"]\n",
        "        self.image_ids = [caption[\"image_id\"] for caption in captions_raw]\n",
        "        self.captions = [caption['caption'] for caption in captions_raw]\n",
        "\n",
        "        if os.path.isfile(f\"{data_path[:-4]}_tokens.pkl\"):\n",
        "            with open(f\"{data_path[:-4]}_tokens.pkl\", 'rb') as f:\n",
        "                self.captions_tokens, self.caption2embedding, self.max_seq_len = pickle.load(f)\n",
        "        else:\n",
        "            self.captions_tokens = []\n",
        "            self.caption2embedding = []\n",
        "            max_seq_len = 0\n",
        "            for caption in captions_raw:\n",
        "                # Tokenize the captions using Hugging Face tokenizer\n",
        "                tokens = self.processor.tokenizer.encode(caption['caption'], truncation=True, padding='max_length')\n",
        "                self.captions_tokens.append(tokens)\n",
        "                self.caption2embedding.append(caption[\"clip_embedding\"])\n",
        "                max_seq_len = max(max_seq_len, len(tokens))\n",
        "\n",
        "            with open(f\"{data_path[:-4]}_tokens.pkl\", 'wb') as f:\n",
        "                pickle.dump([self.captions_tokens, self.caption2embedding, max_seq_len], f)\n",
        "\n",
        "        # Calculate the maximum sequence length\n",
        "        all_len = np.array([len(self.captions_tokens[i]) for i in range(len(self))], dtype=float)\n",
        "        self.max_seq_len = min(int(all_len.mean() + all_len.std() * 10), int(all_len.max()))\n",
        "        self.max_length = max(len(caption) for caption in self.captions_tokens)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.captions_tokens)\n",
        "\n",
        "    def pad_tokens(self, tokens):\n",
        "        # Padding logic to handle sequences\n",
        "        padding = self.max_length - len(tokens)  # Calculate padding\n",
        "        # Convert the padding list to a Tensor\n",
        "        padding_tensor = tf.zeros(padding, dtype=tf.int32)\n",
        "\n",
        "        # Concatenate the original tokens with the padding\n",
        "        tokens = tf.concat([tokens, padding_tensor], axis=0)\n",
        "\n",
        "        return tokens\n",
        "\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        tokens, mask = self.pad_tokens(self.captions_tokens[item])\n",
        "        prefix = self.prefixes[self.caption2embedding[item]]\n",
        "\n",
        "        # Normalize the prefix if required\n",
        "        if self.normalize_prefix:\n",
        "            prefix = prefix / np.linalg.norm(prefix)  # Normalize prefix\n",
        "\n",
        "        return {\n",
        "            'tokens': tf.convert_to_tensor(tokens, dtype=tf.int32),\n",
        "            'mask': tf.convert_to_tensor(mask, dtype=tf.float32),\n",
        "            'prefix': tf.convert_to_tensor(prefix, dtype=tf.float32)\n",
        "        }\n",
        "\n",
        "    def create_tf_dataset(self, batch_size=32):\n",
        "        # Create a TensorFlow dataset from the data\n",
        "        def generator():\n",
        "            for i in range(len(self)):\n",
        "                yield self[i]\n",
        "\n",
        "        dataset = tf.data.Dataset.from_generator(generator,\n",
        "                                                 output_signature={\n",
        "                                                     'tokens': tf.TensorSpec(shape=(self.max_seq_len,), dtype=tf.int32),\n",
        "                                                     'mask': tf.TensorSpec(shape=(self.max_seq_len,), dtype=tf.float32),\n",
        "                                                     'prefix': tf.TensorSpec(shape=(512,), dtype=tf.float32)  # Adjust the prefix dimension\n",
        "                                                 })\n",
        "\n",
        "        # Batching, shuffling, and prefetching\n",
        "        dataset = dataset.batch(batch_size).shuffle(1000).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "        return dataset\n",
        "\n",
        "    @property\n",
        "    def element_spec(self):\n",
        "        # Define the shape and dtype of each element in the dataset\n",
        "        return {\n",
        "            'tokens': tf.TensorSpec(shape=(self.max_seq_len,), dtype=tf.int32),\n",
        "            'mask': tf.TensorSpec(shape=(self.max_seq_len,), dtype=tf.float32),\n",
        "            'prefix': tf.TensorSpec(shape=(512,), dtype=tf.float32)  # Adjust the prefix dimension\n",
        "        }\n",
        "\n",
        "\n",
        "class MLP(Model):\n",
        "    def __init__(self, sizes, bias=True, act=tanh):\n",
        "        super(MLP, self).__init__()\n",
        "        layers_list = []\n",
        "        for i in range(len(sizes) - 1):\n",
        "            layers_list.append(layers.Dense(sizes[i + 1], use_bias=bias))\n",
        "            if i < len(sizes) - 2:\n",
        "                layers_list.append(act)\n",
        "        self.model = tf.keras.Sequential(layers_list)\n",
        "\n",
        "    def call(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "\n",
        "# MlpTransformer Model in TensorFlow\n",
        "class MlpTransformer(Model):\n",
        "    def __init__(self, in_dim, h_dim, out_d=None, act=relu, dropout=0.):\n",
        "        super(MlpTransformer, self).__init__()\n",
        "        out_d = out_d if out_d is not None else in_dim\n",
        "        self.fc1 = layers.Dense(h_dim)\n",
        "        self.act = act\n",
        "        self.fc2 = layers.Dense(out_d)\n",
        "        self.dropout = layers.Dropout(dropout)\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# MultiHeadAttention in TensorFlow\n",
        "class MultiHeadAttention(Model):\n",
        "    def __init__(self, dim_self, dim_ref, num_heads, bias=True, dropout=0.):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim_self // num_heads\n",
        "        self.scale = head_dim ** -0.5\n",
        "\n",
        "        self.to_queries = layers.Dense(dim_self, use_bias=bias)\n",
        "        self.to_keys_values = layers.Dense(dim_ref * 2, use_bias=bias)\n",
        "        self.project = layers.Dense(dim_self)\n",
        "        self.dropout = layers.Dropout(dropout)\n",
        "\n",
        "    def call(self, x, y=None, mask=None):\n",
        "        y = y if y is not None else x\n",
        "        b, n, c = x.shape\n",
        "        _, m, d = y.shape\n",
        "        # Calculate queries and keys-values\n",
        "        queries = self.to_queries(x)\n",
        "        queries = tf.reshape(queries, (b, n, self.num_heads, c // self.num_heads))\n",
        "        keys_values = self.to_keys_values(y)\n",
        "        keys_values = tf.reshape(keys_values, (b, m, 2, self.num_heads, c // self.num_heads))\n",
        "        keys, values = keys_values[:, :, 0], keys_values[:, :, 1]\n",
        "\n",
        "        # Calculate attention\n",
        "        attention = tf.einsum('bnhd,bmhd->bnmh', queries, keys) * self.scale\n",
        "        if mask is not None:\n",
        "            if len(mask.shape) == 2:\n",
        "                mask = tf.expand_dims(mask, 1)\n",
        "            attention = attention + (mask * -1e9)\n",
        "\n",
        "        attention = tf.nn.softmax(attention, axis=2)\n",
        "\n",
        "        # Apply attention to values\n",
        "        out = tf.einsum('bnmh,bmhd->bnhd', attention, values)\n",
        "        out = tf.reshape(out, (b, n, c))\n",
        "        out = self.project(out)\n",
        "        out = self.dropout(out)\n",
        "        return out, attention\n",
        "\n",
        "\n",
        "class ClipCaptionModel(Model):\n",
        "    def get_dummy_token(self, batch_size: int) -> tf.Tensor:\n",
        "        return tf.zeros((batch_size, self.prefix_length), dtype=tf.int64)\n",
        "\n",
        "    def call(self, tokens: tf.Tensor, prefix: tf.Tensor, mask: Optional[tf.Tensor] = None,\n",
        "             labels: Optional[tf.Tensor] = None):\n",
        "        # Embedding for tokens\n",
        "        embedding_text = self.gpt.transformer.wte(tokens)\n",
        "\n",
        "        # Prefix projection\n",
        "        prefix_projections = self.clip_project(prefix)\n",
        "        prefix_projections = tf.reshape(prefix_projections, (-1, self.prefix_length, self.gpt_embedding_size))\n",
        "\n",
        "        # Concatenate prefix projections with the token embeddings\n",
        "        embedding_cat = tf.concat([prefix_projections, embedding_text], axis=1)\n",
        "\n",
        "        # Prepare labels by adding dummy token if labels are provided\n",
        "        if labels is not None:\n",
        "            dummy_token = self.get_dummy_token(tokens.shape[0])\n",
        "            labels = tf.concat([dummy_token, tokens], axis=1)\n",
        "\n",
        "        # Forward pass through the GPT model\n",
        "        out = self.gpt(inputs_embeds=embedding_cat, labels=labels, attention_mask=mask)\n",
        "        return out\n",
        "\n",
        "    def __init__(self, prefix_length: int, clip_length: Optional[int] = None, prefix_size: int = 512,\n",
        "                 num_layers: int = 8, mapping_type='MLP'):\n",
        "        super(ClipCaptionModel, self).__init__()\n",
        "        self.prefix_length = prefix_length\n",
        "        self.gpt = TFGPT2LMHeadModel.from_pretrained('gpt2')\n",
        "        self.gpt_embedding_size = self.gpt.transformer.wte.embeddings.shape[1]\n",
        "\n",
        "\n",
        "        # Mapping type condition for the prefix projection\n",
        "        if mapping_type == 'MLP':\n",
        "            self.clip_project = self.build_mlp(prefix_size)\n",
        "        else:\n",
        "            self.clip_project = self.build_transformer_mapper(prefix_size, self.gpt_embedding_size, prefix_length,\n",
        "                                                                clip_length, num_layers)\n",
        "\n",
        "    def build_mlp(self, prefix_size: int):\n",
        "        model = tf.keras.Sequential([\n",
        "            layers.Dense((self.gpt_embedding_size * self.prefix_length) // 2, activation='relu'),\n",
        "            layers.Dense(self.gpt_embedding_size * self.prefix_length)\n",
        "        ])\n",
        "        return model\n",
        "\n",
        "    def build_transformer_mapper(self, prefix_size: int, gpt_embedding_size: int, prefix_length: int,\n",
        "                                  clip_length: Optional[int], num_layers: int):\n",
        "        # Custom transformer mapper (you may want to define it or use an equivalent)\n",
        "        # This is just a placeholder, you can adjust it as needed\n",
        "        return tf.keras.Sequential([\n",
        "            layers.Dense(prefix_size, activation='relu'),\n",
        "            layers.LayerNormalization(),\n",
        "            layers.MultiHeadAttention(num_heads=4, key_dim=gpt_embedding_size),\n",
        "            layers.Dropout(0.1),\n",
        "            layers.Dense(prefix_size)\n",
        "        ])\n",
        "\n",
        "\n",
        "class ClipCaptionPrefix(ClipCaptionModel):\n",
        "\n",
        "    def train(self, mode: bool = True):\n",
        "        super(ClipCaptionPrefix, self).train(mode)\n",
        "        self.gpt.trainable = False  # Freeze GPT layers\n",
        "        return self\n",
        "\n",
        "\n",
        "\n",
        "def save_config(args: argparse.Namespace):\n",
        "    config = {}\n",
        "    for key, item in args._get_kwargs():\n",
        "        config[key] = item\n",
        "    out_path = os.path.join(args.out_dir, f\"{args.prefix}.json\")\n",
        "    with open(out_path, 'w') as outfile:\n",
        "        json.dump(config, outfile)\n",
        "\n",
        "\n",
        "def load_model(config_path: str, epoch_or_latest: Union[str, int] = '_latest'):\n",
        "    with open(config_path) as f:\n",
        "        config = json.load(f)\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.set_defaults(**config)\n",
        "    args = parser.parse_args()\n",
        "    if type(epoch_or_latest) is int:\n",
        "        epoch_or_latest = f\"-{epoch_or_latest:03d}\"\n",
        "    model_path = os.path.join(args.out_dir, f\"{args.prefix}{epoch_or_latest}.pt\")\n",
        "    model = ClipCaptionGPT2Model(args.prefix_length)\n",
        "    if os.path.isfile(model_path):\n",
        "        print(f\"loading model from {model_path}\")\n",
        "        model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
        "    else:\n",
        "        print(f\"{model_path} does not exist\")\n",
        "    return model, parser\n",
        "\n",
        "\n",
        "def train(dataset, model, args, lr: float = 2e-5, warmup_steps: int = 5000, output_dir: str = \".\", output_prefix: str = \"\"):\n",
        "    batch_size = args.bs\n",
        "    epochs = args.epochs\n",
        "\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    # Initialize optimizer\n",
        "    optimizer = Adam(learning_rate=lr)\n",
        "\n",
        "    # Assuming dataset is already in tf.data.Dataset format\n",
        "    train_dataset = dataset.create_tf_dataset(batch_size=batch_size)\n",
        "\n",
        "    # Define the number of steps per epoch\n",
        "    steps_per_epoch = args.epochs  # If you have a fixed number of steps per epoch, specify this value\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(epochs):\n",
        "        print(f\">>> Training epoch {epoch}\")\n",
        "        sys.stdout.flush()\n",
        "        progress = tqdm(total=steps_per_epoch, desc=output_prefix)\n",
        "\n",
        "        for idx, (images, text_inputs, labels) in enumerate(train_dataset):\n",
        "            with tf.GradientTape() as tape:\n",
        "                # Model forward pass\n",
        "                outputs = model(input_ids=text_inputs, pixel_values=images, labels=labels, return_dict=True)\n",
        "                logits_per_image = outputs.logits_per_image  # Image-text similarity\n",
        "                logits_per_text = outputs.logits_per_text  # Text-image similarity\n",
        "                loss = outputs.loss  # Contrastive loss\n",
        "\n",
        "                # Compute gradients and update weights\n",
        "                gradients = tape.gradient(loss, model.trainable_variables)\n",
        "                optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "            progress.set_postfix({\"loss\": loss.numpy()})\n",
        "            progress.update()\n",
        "\n",
        "            if (idx + 1) % 100 == 0:  # Adjust checkpoint frequency\n",
        "                model.save_pretrained(os.path.join(output_dir, f\"{output_prefix}_latest\"))\n",
        "\n",
        "        progress.close()\n",
        "\n",
        "        # Save the model every 'save_every' epochs\n",
        "        if epoch % args.save_every == 0 or epoch == epochs - 1:\n",
        "            model.save_pretrained(os.path.join(output_dir, f\"{output_prefix}-{epoch:03d}\"))\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    class Args:\n",
        "        def __init__(self):\n",
        "            self.data = 'oscar_split_ViT-B_32_train.pkl'\n",
        "            self.out_dir = './checkpoints'\n",
        "            self.prefix = 'coco_prefix'\n",
        "            self.epochs = 10\n",
        "            self.save_every = 1\n",
        "            self.prefix_length = 10\n",
        "            self.prefix_length_clip = 10\n",
        "            self.bs = 40\n",
        "            self.only_prefix = False\n",
        "            self.mapping_type = 'mlp'\n",
        "            self.num_layers = 8\n",
        "            self.is_rn = False\n",
        "            self.normalize_prefix = False\n",
        "\n",
        "    args = Args()\n",
        "\n",
        "    # Dataset and model setup\n",
        "    dataset = ClipCocoDataset(args.data, args.prefix_length, normalize_prefix=args.normalize_prefix)\n",
        "    prefix_dim = 640 if args.is_rn else 512\n",
        "    args.mapping_type = {'mlp': MappingType.MLP, 'transformer': MappingType.Transformer}[args.mapping_type]\n",
        "\n",
        "    if args.only_prefix:\n",
        "        model = ClipCaptionPrefix(args.prefix_length, clip_length=args.prefix_length_clip,\n",
        "                                  prefix_size=prefix_dim, num_layers=args.num_layers,\n",
        "                                  mapping_type=args.mapping_type)\n",
        "        print(\"Train only prefix\")\n",
        "    else:\n",
        "        model = ClipCaptionModel(args.prefix_length, clip_length=args.prefix_length_clip,\n",
        "                                 prefix_size=prefix_dim, num_layers=args.num_layers,\n",
        "                                 mapping_type=args.mapping_type)\n",
        "        print(\"Train both prefix and GPT\")\n",
        "\n",
        "    # Start training\n",
        "    train(dataset, model, args, output_dir=args.out_dir, output_prefix=args.prefix)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "BIU_vfUXlVL1",
        "outputId": "46212030-f7d7-4352-9303-235eb7c33290"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data size is 56674\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
            "\n",
            "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train both prefix and GPT\n",
            ">>> Training epoch 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "\n",
            "coco_prefix:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A"
          ]
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "{{function_node __wrapped__IteratorGetNext_output_types_3_device_/job:localhost/replica:0/task:0/device:CPU:0}} ValueError: too many values to unpack (expected 2)\nTraceback (most recent call last):\n\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/script_ops.py\", line 269, in __call__\n    ret = func(*args)\n          ^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/from_generator_op.py\", line 198, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"<ipython-input-37-618bae5c5c12>\", line 94, in generator\n    yield self[i]\n          ~~~~^^^\n\n  File \"<ipython-input-37-618bae5c5c12>\", line 77, in __getitem__\n    tokens, mask = self.pad_tokens(self.captions_tokens[item])\n    ^^^^^^^^^^^^\n\nValueError: too many values to unpack (expected 2)\n\n\n\t [[{{node PyFunc}}]] [Op:IteratorGetNext] name: ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-618bae5c5c12>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-37-618bae5c5c12>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m     \u001b[0;31m# Start training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_prefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-37-618bae5c5c12>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataset, model, args, lr, warmup_steps, output_dir, output_prefix)\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0mprogress\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_prefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m                 \u001b[0;31m# Model forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    824\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 826\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    827\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    774\u001b[0m     \u001b[0;31m# to communicate that there is no more data to iterate over.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecution_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSYNC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 776\u001b[0;31m       ret = gen_dataset_ops.iterator_get_next(\n\u001b[0m\u001b[1;32m    777\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m           \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   3084\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3085\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3086\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3087\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3088\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6000\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mNoReturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6001\u001b[0m   \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6002\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6003\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__IteratorGetNext_output_types_3_device_/job:localhost/replica:0/task:0/device:CPU:0}} ValueError: too many values to unpack (expected 2)\nTraceback (most recent call last):\n\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/script_ops.py\", line 269, in __call__\n    ret = func(*args)\n          ^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/from_generator_op.py\", line 198, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"<ipython-input-37-618bae5c5c12>\", line 94, in generator\n    yield self[i]\n          ~~~~^^^\n\n  File \"<ipython-input-37-618bae5c5c12>\", line 77, in __getitem__\n    tokens, mask = self.pad_tokens(self.captions_tokens[item])\n    ^^^^^^^^^^^^\n\nValueError: too many values to unpack (expected 2)\n\n\n\t [[{{node PyFunc}}]] [Op:IteratorGetNext] name: "
          ]
        }
      ]
    }
  ]
}