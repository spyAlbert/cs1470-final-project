# -*- coding: utf-8 -*-
"""clip_preprocess_tf.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zS6ey8yQLi0FzCsctm_jGpeFJp9DJbQf
"""

import os
import json
import pickle
import argparse
from pathlib import Path

import tensorflow as tf
from PIL import Image
from tqdm import tqdm
from transformers import CLIPProcessor, TFCLIPModel

import numpy as np

def load_json(path: Path):
    """Read caption json to memory."""
    with path.open("r") as fp:
        captions = json.load(fp)
    print(f"{len(captions)} captions loaded from json")
    return captions

def find_image(img_id: int) -> Path:
    """
    Given a COCO image id, locate the JPG file in either train2014 or val2014 folder.
    """
    try:
        img_id = int(img_id)
    except ValueError:
        raise ValueError(f"Image id {img_id} is not a valid integer.")

    filename = f"COCO_train2014_{img_id:012d}.jpg"
    train_path = Path("./data/coco_sampled/train2014") / filename
    if train_path.is_file():
        return train_path

    filename = f"COCO_val2014_{img_id:012d}.jpg"
    val_path = Path("./data/coco_sampled/val2014") / filename
    if val_path.is_file():
        return val_path


def extract_embeddings(model, processor, captions) -> tuple[list[np.ndarray], list[dict]]:
    """
    Iterate over captions, load corresponding images,
    and compute CLIP image embeddings.
    """
    features, meta = [], []

    for idx in tqdm(range(len(captions))):
        item = captions[idx]
        img_path = find_image(item["image_id"])
        if img_path is None:
            tqdm.write(f"Skipping missing image {item['image_id']}")
            continue

        image = Image.open(img_path).convert("RGB")
        inputs = processor(images=image, return_tensors="tf")

        outputs = model.get_image_features(**inputs)
        emb = outputs.numpy()

        item["clip_embedding"] = idx
        features.append(emb)
        meta.append(item)

        # checkpoint every 10k samples
        if (idx + 1) % 10_000 == 0:
            yield idx + 1, features, meta
            features, meta = [], []

    # final remainder
    yield len(captions), features, meta

def dump_checkpoint(out_path: Path, feats: list[np.ndarray], infos: list[dict]):
    """Serialize current batch to a pkl file."""
    tensor_cat = np.concatenate(feats, axis=0)
    payload = {"clip_embedding": tensor_cat, "captions": infos}
    with out_path.open("wb") as fp:
        pickle.dump(payload, fp)

def run(model_name: str):
    # Use GPU if available
    print("Num GPUs Available:", len(tf.config.list_physical_devices('GPU')))

    from transformers import TFAutoModel, AutoTokenizer

    model_name = "openai/clip-vit-base-patch32"  # Correct Hugging Face model ID

    model = TFAutoModel.from_pretrained(model_name)
    tokenizer = AutoTokenizer.from_pretrained(model_name)

    processor = CLIPProcessor.from_pretrained(model_name)

    output_pkl = Path(f"./data/coco/oscar_split_{model_name.replace('/', '_')}_train.pkl")

    caption_file = Path("train_caption.json")
    captions = load_json(caption_file)

    progress = 0
    for progress, feats, infos in extract_embeddings(model, processor, captions):
        dump_checkpoint(output_pkl, feats, infos)

    print("Done")
    print(f"{progress} embeddings saved to {output_pkl}")

if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument('--clip_model_name', type=str, default="ViT-B/32")
    args = parser.parse_known_args()[0]
    clip_model_name = args.clip_model_name
    run(args.clip_model_name)