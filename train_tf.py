# -*- coding: utf-8 -*-
"""train_tf.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ONFwEgnxDoWZRoyeR2eqUyVjWCvZSSFE
"""

import tensorflow as tf
from transformers import GPT2Model, GPT2Tokenizer, CLIPModel, CLIPProcessor
from transformers import TFGPT2LMHeadModel, GPT2Tokenizer
import pickle
import os
import argparse
import json
from typing import Tuple, Optional
from typing import Union
from enum import Enum
from tensorflow.keras import layers, Model
from tensorflow.keras.activations import tanh, relu
from tensorflow.keras.optimizers import Adam
import sys
from tqdm import tqdm

class MappingType(Enum):
    MLP = 'mlp'
    Transformer = 'transformer'


class ClipCocoDataset:
    def __init__(self, data_path: str, prefix_length: int, normalize_prefix=False):
        # Initialize tokenizer and dataset parameters
        self.processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch16")
        self.prefix_length = prefix_length
        self.normalize_prefix = normalize_prefix

        # Load dataset
        with open(data_path, 'rb') as f:
            all_data = pickle.load(f)
        print("Data size is %0d" % len(all_data["clip_embedding"]))

        self.prefixes = all_data["clip_embedding"]
        captions_raw = all_data["captions"]
        self.image_ids = [caption["image_id"] for caption in captions_raw]
        self.captions = [caption['caption'] for caption in captions_raw]

        if os.path.isfile(f"{data_path[:-4]}_tokens.pkl"):
            with open(f"{data_path[:-4]}_tokens.pkl", 'rb') as f:
                self.captions_tokens, self.caption2embedding, self.max_seq_len = pickle.load(f)
        else:
            self.captions_tokens = []
            self.caption2embedding = []
            max_seq_len = 0
            for caption in captions_raw:
                # Tokenize the captions using Hugging Face tokenizer
                tokens = self.processor.tokenizer.encode(caption['caption'], truncation=True, padding='max_length')
                self.captions_tokens.append(tokens)
                self.caption2embedding.append(caption["clip_embedding"])
                max_seq_len = max(max_seq_len, len(tokens))

            with open(f"{data_path[:-4]}_tokens.pkl", 'wb') as f:
                pickle.dump([self.captions_tokens, self.caption2embedding, max_seq_len], f)

        # Calculate the maximum sequence length
        all_len = np.array([len(self.captions_tokens[i]) for i in range(len(self))], dtype=float)
        self.max_seq_len = min(int(all_len.mean() + all_len.std() * 10), int(all_len.max()))
        self.max_length = max(len(caption) for caption in self.captions_tokens)

    def __len__(self):
        return len(self.captions_tokens)

    def pad_tokens(self, tokens):
        padding = self.max_length - len(tokens)
        padding_tensor = tf.zeros(padding, dtype=tf.int32)
        tokens_padded = tf.concat([tokens, padding_tensor], axis=0)

        mask = tf.concat([tf.ones(len(tokens), dtype=tf.int32), tf.zeros(padding, dtype=tf.int32)], axis=0)

        return tokens_padded, mask


    def __getitem__(self, item):
        tokens, mask = self.pad_tokens(self.captions_tokens[item])
        prefix = self.prefixes[self.caption2embedding[item]]

        if self.normalize_prefix:
            prefix = prefix / np.linalg.norm(prefix)

        return {
            'tokens': tf.convert_to_tensor(tokens, dtype=tf.int32),
            'mask': tf.cast(mask, dtype=tf.float32),
            'prefix': tf.convert_to_tensor(prefix, dtype=tf.float32)
        }

    def create_tf_dataset(self, batch_size=32):
        # Create a TensorFlow dataset from the data
        def generator():
            for i in range(len(self)):
                yield self[i]

        dataset = tf.data.Dataset.from_generator(generator,
                                                 output_signature={
                                                     'tokens': tf.TensorSpec(shape=(self.max_seq_len,), dtype=tf.int32),
                                                     'mask': tf.TensorSpec(shape=(self.max_seq_len,), dtype=tf.float32),
                                                     'prefix': tf.TensorSpec(shape=(512,), dtype=tf.float32)  # Adjust the prefix dimension
                                                 })

        # Batching, shuffling, and prefetching
        dataset = dataset.batch(batch_size).shuffle(1000).prefetch(tf.data.experimental.AUTOTUNE)
        return dataset

    @property
    def element_spec(self):
        return {
            'tokens': tf.TensorSpec(shape=(self.max_seq_len,), dtype=tf.int32),
            'mask': tf.TensorSpec(shape=(self.max_seq_len,), dtype=tf.float32),
            'prefix': tf.TensorSpec(shape=(512,), dtype=tf.float32)
        }


class MLP(Model):
    def __init__(self, sizes, bias=True, act=tanh):
        super(MLP, self).__init__()
        layers_list = []
        for i in range(len(sizes) - 1):
            layers_list.append(layers.Dense(sizes[i + 1], use_bias=bias))
            if i < len(sizes) - 2:
                layers_list.append(act)
        self.model = tf.keras.Sequential(layers_list)

    def call(self, x):
        return self.model(x)


# MlpTransformer Model
class MlpTransformer(Model):
    def __init__(self, in_dim, h_dim, out_d=None, act=relu, dropout=0.):
        super(MlpTransformer, self).__init__()
        out_d = out_d if out_d is not None else in_dim
        self.fc1 = layers.Dense(h_dim)
        self.act = act
        self.fc2 = layers.Dense(out_d)
        self.dropout = layers.Dropout(dropout)

    def call(self, x):
        x = self.fc1(x)
        x = self.act(x)
        x = self.dropout(x)
        x = self.fc2(x)
        x = self.dropout(x)
        return x


# MultiHeadAttention
class MultiHeadAttention(Model):
    def __init__(self, dim_self, dim_ref, num_heads, bias=True, dropout=0.):
        super(MultiHeadAttention, self).__init__()
        self.num_heads = num_heads
        head_dim = dim_self // num_heads
        self.scale = head_dim ** -0.5

        self.to_queries = layers.Dense(dim_self, use_bias=bias)
        self.to_keys_values = layers.Dense(dim_ref * 2, use_bias=bias)
        self.project = layers.Dense(dim_self)
        self.dropout = layers.Dropout(dropout)

    def call(self, x, y=None, mask=None):
        y = y if y is not None else x
        b, n, c = x.shape
        _, m, d = y.shape
        # Calculate queries and keys-values
        queries = self.to_queries(x)
        queries = tf.reshape(queries, (b, n, self.num_heads, c // self.num_heads))
        keys_values = self.to_keys_values(y)
        keys_values = tf.reshape(keys_values, (b, m, 2, self.num_heads, c // self.num_heads))
        keys, values = keys_values[:, :, 0], keys_values[:, :, 1]

        # Calculate attention
        attention = tf.einsum('bnhd,bmhd->bnmh', queries, keys) * self.scale
        if mask is not None:
            if len(mask.shape) == 2:
                mask = tf.expand_dims(mask, 1)
            attention = attention + (mask * -1e9)

        attention = tf.nn.softmax(attention, axis=2)

        # Apply attention to values
        out = tf.einsum('bnmh,bmhd->bnhd', attention, values)
        out = tf.reshape(out, (b, n, c))
        out = self.project(out)
        out = self.dropout(out)
        return out, attention


class ClipCaptionModel(Model):
    def get_dummy_token(self, batch_size: int) -> tf.Tensor:
        return tf.zeros((batch_size, self.prefix_length), dtype=tf.int32)

    def call(self,
             inputs,
             mask: Optional[tf.Tensor]   = None,
             labels: Optional[tf.Tensor] = None):

        tokens = inputs['tokens']
        prefix = inputs['prefix']

        # Embedding for tokens
        embedding_text = self.gpt.transformer.wte(tokens)
        prefix_projections = self.clip_project(prefix)
        prefix_projections = self.prefix_proj_to_gpt(prefix_projections)
        embedding_cat = tf.concat([prefix_projections, embedding_text], axis=1)

        if labels is not None:
            dummy_token = self.get_dummy_token(tokens.shape[0])
            labels = tf.concat([dummy_token, tokens], axis=1)

        out = self.gpt(inputs_embeds=embedding_cat, labels=labels, attention_mask=mask)
        return out

    def __init__(self, prefix_length: int, clip_length: Optional[int] = None, prefix_size: int = 512,
                 num_layers: int = 8, mapping_type='MLP'):
        super(ClipCaptionModel, self).__init__()
        self.prefix_length = prefix_length
        self.gpt = TFGPT2LMHeadModel.from_pretrained('gpt2')
        self.gpt_embedding_size = self.gpt.transformer.wte.embeddings.shape[1]
        # Mapping type condition for the prefix projection
        if mapping_type == 'MLP':
            self.clip_project = self.build_mlp(prefix_size)
        else:
            self.clip_project = self.build_transformer_mapper(
    prefix_size, self.gpt_embedding_size, prefix_length, clip_length, num_layers)
        self.prefix_proj_to_gpt = layers.Dense(self.gpt_embedding_size)


    def build_mlp(self, prefix_size: int):
        inp = tf.keras.Input(shape=(prefix_size,))
        x = layers.Dense(
            units=self.gpt_embedding_size * self.prefix_length,
            activation='relu'
        )(inp)

        x = layers.Reshape((self.prefix_length, self.gpt_embedding_size))(x)

        return tf.keras.Model(inputs=inp, outputs=x, name="prefix_mlp_mapper")

    def build_transformer_mapper(self, prefix_size, gpt_embedding_size,
                             prefix_length, clip_length, num_layers):
      inputs = tf.keras.Input(shape=(prefix_size,))
      x = layers.Dense(prefix_size, activation='relu')(inputs)
      x = layers.LayerNormalization()(x)
      x = layers.Dense(gpt_embedding_size * prefix_length)(x)
      x = layers.Reshape((prefix_length, gpt_embedding_size))(x)

      x = layers.MultiHeadAttention(
          num_heads=4,
          key_dim=gpt_embedding_size
      )(query=x, value=x)
      x = layers.Dropout(0.1)(x)
      out = layers.Dense(prefix_size)(x)

      return tf.keras.Model(inputs=inputs, outputs=out)


class ClipCaptionPrefix(ClipCaptionModel):

    def train(self, mode: bool = True):
        super(ClipCaptionPrefix, self).train(mode)
        self.gpt.trainable = False  # Freeze GPT layers
        return self


def save_config(args: argparse.Namespace):
    config = {}
    for key, item in args._get_kwargs():
        config[key] = item
    out_path = os.path.join(args.out_dir, f"{args.prefix}.json")
    with open(out_path, 'w') as outfile:
        json.dump(config, outfile)


def load_model(config_path: str, epoch_or_latest: Union[str, int] = '_latest'):
    with open(config_path) as f:
        config = json.load(f)

    parser = argparse.ArgumentParser()
    parser.set_defaults(**config)
    args = parser.parse_args()

    if isinstance(epoch_or_latest, int):
        epoch_or_latest = f"-{epoch_or_latest:03d}"

    model_path = os.path.join(args.out_dir, f"{args.prefix}{epoch_or_latest}.h5")

    if os.path.isfile(model_path):
        print(f"Loading model from {model_path}")
        model = tf.keras.models.load_model(model_path)
    else:
        print(f"{model_path} does not exist")
        model = None

    return model, parser



import math

def train(dataset, model, args, lr: float = 2e-5, warmup_steps: int = 5000,
          output_dir: str = ".", output_prefix: str = ""):
    batch_size = args.bs
    epochs     = args.epochs

    os.makedirs(output_dir, exist_ok=True)
    optimizer = Adam(learning_rate=lr)
    train_dataset = dataset.create_tf_dataset(batch_size=batch_size)

    # Compute steps_per_epoch from dataset length and batch size
    dataset_size    = len(dataset)
    steps_per_epoch = math.ceil(dataset_size / batch_size)

    for epoch in range(epochs):
        print(f">>> Training epoch {epoch+1}/{epochs}")
        sys.stdout.flush()
        progress = tqdm(total=steps_per_epoch, desc=output_prefix)

        for batch in train_dataset:
            inputs = {
                'tokens': batch['tokens'],
                'prefix': batch['prefix']
            }
            mask   = batch['mask']
            labels = batch['tokens']

            with tf.GradientTape() as tape:
                outputs = model(inputs, mask=mask, labels=labels)
                loss = outputs.loss


            grads = tape.gradient(loss, model.trainable_variables)
            optimizer.apply_gradients(zip(grads, model.trainable_variables))

            progress.set_postfix({"loss": float(loss)})
            progress.update(1)

            # periodically save the latest
            step = int(progress.n)
            if step > 0 and step % 100 == 0:
                model.save(os.path.join(output_dir, f"{output_prefix}_latest"))

        progress.close()

        # save per-epoch checkpoint
        if epoch % args.save_every == 0 or epoch == epochs - 1:
            model.save(os.path.join(output_dir, f"{output_prefix}-{epoch:03d}"))

    return model


def main():
    class Args:
        def __init__(self):
            self.data = 'oscar_split_ViT-B_32_train.pkl'
            self.out_dir = './checkpoints'
            self.prefix = 'coco_prefix'
            self.epochs = 10
            self.save_every = 1
            self.prefix_length = 10
            self.prefix_length_clip = 10
            self.bs = 40
            self.only_prefix = False
            self.mapping_type = 'mlp'
            self.num_layers = 8
            self.is_rn = False
            self.normalize_prefix = False

    args = Args()

    # Dataset and model setup
    dataset = ClipCocoDataset(args.data, args.prefix_length, normalize_prefix=args.normalize_prefix)
    prefix_dim = 640 if args.is_rn else 512
    args.mapping_type = {'mlp': MappingType.MLP, 'transformer': MappingType.Transformer}[args.mapping_type]

    if args.only_prefix:
        model = ClipCaptionPrefix(args.prefix_length, clip_length=args.prefix_length_clip,
                                  prefix_size=prefix_dim, num_layers=args.num_layers,
                                  mapping_type=args.mapping_type)

        print("Train only prefix")
    else:
        model = ClipCaptionModel(args.prefix_length, clip_length=args.prefix_length_clip,
                                 prefix_size=prefix_dim, num_layers=args.num_layers,
                                 mapping_type=args.mapping_type)
        print("Train both prefix and GPT")

    # Start training
    train(dataset, model, args, output_dir=args.out_dir, output_prefix=args.prefix)

if __name__ == '__main__':
    main()