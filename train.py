# -*- coding: utf-8 -*-
"""train.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ISWNdF84dAgqxwIefil33OuJd3G69eEz
"""

import tensorflow as tf
from transformers import TFGPT2LMHeadModel, GPT2Tokenizer
import os
import pickle
import sys
import numpy as np
from enum import Enum
from typing import Tuple, Optional
import argparse
import sys


class MappingType(Enum):
    MLP = 'mlp'
    Transformer = 'transformer'


class ClipCocoDataset(tf.data.Dataset):
    def __new__(cls, data_path: str, prefix_length: int, gpt2_type: str = "gpt2", normalize_prefix=False):
        tokenizer = GPT2Tokenizer.from_pretrained(gpt2_type)
        with open(data_path, 'rb') as f:
            all_data = pickle.load(f)

        prefixes = all_data["clip_embedding"]
        captions_raw = all_data["captions"]

        image_ids = [caption["image_id"] for caption in captions_raw]
        captions = [caption['caption'] for caption in captions_raw]

        token_path = f"{data_path[:-4]}_tokens.pkl"
        if os.path.isfile(token_path):
            with open(token_path, 'rb') as f:
                captions_tokens, caption2embedding, max_seq_len = pickle.load(f)
        else:
            captions_tokens = [
                tokenizer.encode(caption['caption']) for caption in captions_raw
            ]
            caption2embedding = [caption["clip_embedding"] for caption in captions_raw]
            max_seq_len = max(len(tokens) for tokens in captions_tokens)
            with open(token_path, 'wb') as f:
                pickle.dump([captions_tokens, caption2embedding, max_seq_len], f)

        all_len = np.array([len(tokens) for tokens in captions_tokens])
        max_seq_len = int(min(all_len.mean() + all_len.std() * 10, all_len.max()))

        def gen():
            for tokens, emb_idx in zip(captions_tokens, caption2embedding):
                tokens = tokens[:max_seq_len] + [-1] * max(0, max_seq_len - len(tokens))
                tokens = np.array(tokens)
                mask = (tokens >= 0).astype(np.float32)
                tokens[tokens < 0] = 0
                mask = np.concatenate([np.ones(prefix_length), mask], axis=0)
                prefix = prefixes[emb_idx]
                if normalize_prefix:
                    prefix = prefix / np.linalg.norm(prefix, axis=-1, keepdims=True)
                yield tokens, mask, prefix

        output_signature = (
            tf.TensorSpec(shape=(max_seq_len,), dtype=tf.int32),
            tf.TensorSpec(shape=(max_seq_len + prefix_length,), dtype=tf.float32),
            tf.TensorSpec(shape=(prefixes[0].shape), dtype=tf.float32)
        )

        return tf.data.Dataset.from_generator(gen, output_signature=output_signature)


class MLP(tf.keras.Model):
    def __init__(self, sizes: Tuple[int, ...], activation=tf.keras.activations.tanh):
        super(MLP, self).__init__()
        self.model = tf.keras.Sequential()
        for i in range(len(sizes) - 1):
            self.model.add(tf.keras.layers.Dense(sizes[i + 1]))
            if i < len(sizes) - 2:
                self.model.add(tf.keras.layers.Activation(activation))

    def call(self, x):
        return self.model(x)

class MlpTransformer(tf.keras.Model):
    def __init__(self, in_dim, h_dim, out_d: Optional[int] = None, act='relu', dropout=0.0):
        super().__init__()
        out_d = out_d if out_d is not None else in_dim
        self.fc1 = layers.Dense(h_dim)
        self.act = tf.keras.activations.get(act)
        self.dropout = layers.Dropout(dropout)
        self.fc2 = layers.Dense(out_d)

    def call(self, x, training=False):
        x = self.fc1(x)
        x = self.act(x)
        x = self.dropout(x, training=training)
        x = self.fc2(x)
        x = self.dropout(x, training=training)
        return x


class MultiHeadAttention(tf.keras.Model):
    def __init__(self, dim_self, dim_ref, num_heads, dropout=0.0):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = dim_self // num_heads
        self.scale = self.head_dim ** -0.5

        self.to_queries = layers.Dense(dim_self)
        self.to_keys_values = layers.Dense(dim_self * 2)
        self.project = layers.Dense(dim_self)
        self.dropout = layers.Dropout(dropout)

    def call(self, x, y=None, mask=None, training=False):
        y = x if y is None else y

        batch_size = tf.shape(x)[0]
        n = tf.shape(x)[1]
        m = tf.shape(y)[1]
        c = x.shape[-1]

        q = self.to_queries(x)
        kv = self.to_keys_values(y)

        q = tf.reshape(q, (batch_size, n, self.num_heads, self.head_dim))
        kv = tf.reshape(kv, (batch_size, m, 2, self.num_heads, self.head_dim))

        k, v = kv[:, :, 0], kv[:, :, 1]

        attn = tf.einsum('bnhd,bmhd->bnmh', q, k) * self.scale

        if mask is not None:
            mask = tf.expand_dims(mask, axis=1) if len(mask.shape) == 2 else mask
            attn += (1.0 - tf.cast(mask[:, :, :, tf.newaxis], tf.float32)) * -1e9

        attn = tf.nn.softmax(attn, axis=2)
        attn = self.dropout(attn, training=training)

        out = tf.einsum('bnmh,bmhd->bnhd', attn, v)
        out = tf.reshape(out, (batch_size, n, c))
        out = self.project(out)
        return out, attn


class TransformerLayer(tf.keras.Model):
    def __init__(self, dim_self, dim_ref, num_heads, mlp_ratio=4.0, dropout=0.0, act='relu'):
        super().__init__()
        self.norm1 = layers.LayerNormalization(epsilon=1e-6)
        self.attn = MultiHeadAttention(dim_self, dim_ref, num_heads, dropout=dropout)
        self.norm2 = layers.LayerNormalization(epsilon=1e-6)
        self.mlp = MlpTransformer(dim_self, int(dim_self * mlp_ratio), act=act, dropout=dropout)

    def call(self, x, y=None, mask=None, training=False):
        attn_out, _ = self.attn(self.norm1(x), y, mask, training=training)
        x = x + attn_out
        x = x + self.mlp(self.norm2(x), training=training)
        return x

    def call_with_attention(self, x, y=None, mask=None, training=False):
        attn_out, attn_scores = self.attn(self.norm1(x), y, mask, training=training)
        x = x + attn_out
        x = x + self.mlp(self.norm2(x), training=training)
        return x, attn_scores
class Transformer(tf.keras.Model):
    def __init__(self, dim_self, num_heads, num_layers, dim_ref=None,
                 mlp_ratio=2., act='relu', enc_dec=False):
        super().__init__()
        dim_ref = dim_ref if dim_ref is not None else dim_self
        self.enc_dec = enc_dec
        self.layers_list = []

        if enc_dec:
            num_layers *= 2

        for i in range(num_layers):
            if enc_dec:
                if i % 2 == 0:
                    layer = TransformerLayer(dim_self, dim_ref, num_heads, mlp_ratio, act)
                else:
                    layer = TransformerLayer(dim_self, dim_self, num_heads, mlp_ratio, act)
            else:
                layer = TransformerLayer(dim_self, dim_ref, num_heads, mlp_ratio, act)
            self.layers_list.append(layer)

    def call(self, x, y=None, mask=None, training=False):
        for i, layer in enumerate(self.layers_list):
            if self.enc_dec:
                if i % 2 == 0:
                    x = layer(x, y, mask, training=training)
                else:
                    x = layer(x, x, mask, training=training)
            else:
                x = layer(x, y, mask, training=training)
        return x

    def call_with_attention(self, x, y=None, mask=None):
        attentions = []
        for layer in self.layers_list:
            x, att = layer(x, y, mask, return_attention=True)
            attentions.append(att)
        return x, attentions


class TransformerMapper(tf.keras.Model):
    def __init__(self, dim_clip, dim_embedding, prefix_length, clip_length, num_layers=8):
        super().__init__()
        self.clip_length = clip_length
        self.linear = layers.Dense(clip_length * dim_embedding)
        self.prefix_const = self.add_weight("prefix_const", shape=(prefix_length, dim_embedding), initializer='random_normal', trainable=True)
        self.transformer = Transformer(dim_embedding, num_heads=8, num_layers=num_layers)

    def call(self, x):
        batch_size = tf.shape(x)[0]
        x = self.linear(x)
        x = tf.reshape(x, (batch_size, self.clip_length, -1))
        prefix = tf.expand_dims(self.prefix_const, 0)
        prefix = tf.repeat(prefix, batch_size, axis=0)
        prefix = tf.concat([x, prefix], axis=1)
        out = self.transformer(prefix)
        return out[:, self.clip_length:]


class ClipCaptionModel(tf.keras.Model):
    def __init__(self, gpt_model, prefix_length, prefix_size=512, clip_length=None,
                 num_layers=8, mapping_type='mlp'):
        super().__init__()
        self.prefix_length = prefix_length
        self.gpt = gpt_model
        self.gpt_embedding_size = self.gpt.config.hidden_size

        if mapping_type == 'mlp':
            self.clip_project = tf.keras.Sequential([
                layers.Dense((self.gpt_embedding_size * prefix_length) // 2, activation='relu'),
                layers.Dense(self.gpt_embedding_size * prefix_length)
            ])
        else:
            self.clip_project = TransformerMapper(prefix_size, self.gpt_embedding_size,
                                                  prefix_length, clip_length, num_layers)

    def get_dummy_token(self, batch_size, device):
        return tf.zeros((batch_size, self.prefix_length), dtype=tf.int64)

    def call(self, tokens, prefix, mask=None, labels=None):
        embedding_text = self.gpt.transformer.wte(tokens)
        prefix_projections = self.clip_project(prefix)
        prefix_projections = tf.reshape(prefix_projections, (-1, self.prefix_length, self.gpt_embedding_size))
        embedding_cat = tf.concat([prefix_projections, embedding_text], axis=1)

        if labels is not None:
            dummy_token = self.get_dummy_token(tf.shape(tokens)[0], tokens.device)
            labels = tf.concat([dummy_token, tokens], axis=1)

        outputs = self.gpt(inputs_embeds=embedding_cat, attention_mask=mask, labels=labels)
        return outputs


class ClipCaptionPrefix(tf.keras.Model):
    def __init__(self, clip_project, gpt_model):
        super(ClipCaptionPrefix, self).__init__()
        self.clip_project = clip_project
        self.gpt = gpt_model
        self.gpt.trainable = False  # Freeze GPT model

    def call(self, inputs, training=False):
        # Define the forward pass
        # inputs: tuple of (tokens, prefix, mask)
        tokens, prefix, mask = inputs
        embedding_text = self.gpt.transformer.wte(tokens)
        prefix_projections = self.clip_project(prefix)
        embedding_cat = tf.concat([prefix_projections, embedding_text], axis=1)
        outputs = self.gpt(inputs_embeds=embedding_cat, attention_mask=mask, training=training)
        return outputs

def save_config(args):
    config = vars(args)
    out_path = os.path.join(args.out_dir, f"{args.prefix}.json")
    with open(out_path, 'w') as outfile:
        json.dump(config, outfile)


def load_model(config_path, epoch_or_latest='_latest'):
    with open(config_path) as f:
        config = json.load(f)
    parser = argparse.ArgumentParser()
    parser.set_defaults(**config)
    args = parser.parse_args()
    if isinstance(epoch_or_latest, int):
        epoch_or_latest = f"-{epoch_or_latest:03d}"
    model_path = os.path.join(args.out_dir, f"{args.prefix}{epoch_or_latest}.h5")
    if args.only_prefix:
        # Define and load ClipCaptionPrefix model
        model = ClipCaptionPrefix(args.prefix_length)
    else:
        # Define and load ClipCaptionModel
        model = ClipCaptionModel(args.prefix_length)
    if os.path.isfile(model_path):
        print(f"Loading model from {model_path}")
        model.load_weights(model_path)
    else:
        print(f"{model_path} does not exist")
    return model, parser


def train(dataset, model, args, lr=2e-5, warmup_steps=5000, output_dir=".", output_prefix=""):
    batch_size = args.bs
    epochs = args.epochs
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)
    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)

    # Prepare the dataset
    train_dataset = dataset.batch(batch_size).shuffle(buffer_size=1024)

    for epoch in range(epochs):
        print(f">>> Training epoch {epoch}")
        progress = tqdm(total=len(train_dataset), desc=output_prefix)
        for step, (tokens, mask, prefix) in enumerate(train_dataset):
            with tf.GradientTape() as tape:
                outputs = model((tokens, prefix, mask), training=True)
                logits = outputs.logits[:, args.prefix_length - 1: -1]
                loss = loss_fn(tokens[:, 1:], logits)
            gradients = tape.gradient(loss, model.trainable_variables)
            optimizer.apply_gradients(zip(gradients, model.trainable_variables))
            progress.set_postfix({"loss": loss.numpy()})
            progress.update(1)
            if (step + 1) % 10000 == 0:
                model.save_weights(os.path.join(output_dir, f"{output_prefix}_latest.h5"))
        progress.close()
        if epoch % args.save_every == 0 or epoch == epochs - 1:
            model.save_weights(os.path.join(output_dir, f"{output_prefix}-{epoch:03d}.h5"))
    return model

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--data', default='./data/coco/oscar_split_train.pkl')
    parser.add_argument('--out_dir', default='./checkpoints')
    parser.add_argument('--prefix', default='coco_prefix', help='prefix for saved filenames')
    parser.add_argument('--epochs', type=int, default=10)
    parser.add_argument('--save_every', type=int, default=1)
    parser.add_argument('--prefix_length', type=int, default=10)
    parser.add_argument('--prefix_length_clip', type=int, default=10)
    parser.add_argument('--bs', type=int, default=40)
    parser.add_argument('--only_prefix', dest='only_prefix', action='store_true')
    parser.add_argument('--mapping_type', type=str, default='mlp', help='mlp/transformer')
    parser.add_argument('--num_layers', type=int, default=8)
    parser.add_argument('--is_rn', dest='is_rn', action='store_true')
    parser.add_argument('--normalize_prefix', dest='normalize_prefix', action='store_true')
    #args = parser.parse_args()
    args, _ = parser.parse_known_args()

    prefix_length = args.prefix_length
    # Load your dataset here
    dataset = ClipCocoDataset(args.data, prefix_length, normalize_prefix=args.normalize_prefix)
    prefix_dim = 640 if args.is_rn else 512
    args.mapping_type = {'mlp': MappingType.MLP, 'transformer': MappingType.Transformer}[args.mapping_type]
    if args.only_prefix:
        model = ClipCaptionPrefix(prefix_length, clip_length=args.prefix_length_clip, prefix_size=prefix_dim,
                                  num_layers=args.num_layers, mapping_type=args.mapping_type)
        print("Train only prefix")
    else:
        model = ClipCaptionModel(prefix_length, clip_length=args.prefix_length_clip, prefix_size=prefix_dim,
                                  num_layers=args.num_layers, mapping_type=args.mapping_type)
        print("Train both prefix and GPT")
        sys.stdout.flush()
    train(dataset, model, args, output_dir=args.out_dir, output_prefix=args.prefix)


if __name__ == '__main__':
    main()